{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf8f3e4e-9379-4eef-a392-134b5a63263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.clustering import GaussianMixture, GaussianMixtureModel\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faac0ed7-093c-4a77-a08e-29bf56a897a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/02 13:11:14 WARN Utils: Your hostname, Bens-MacBook-Air-7.local, resolves to a loopback address: 127.0.0.1; using 172.20.60.48 instead (on interface en0)\n",
      "25/12/02 13:11:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/02 13:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- language_id: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating spark app\n",
    "spark = SparkSession.builder.appName(\"gmm_implementation\").getOrCreate()\n",
    "\n",
    "#reading in the CSV file \n",
    "# local_file = 'numpy_array_for_modeling_with_cathegorical_columns.csv'\n",
    "cloud_fle = 'gs://term-project-fall-202511162025/running_big_data/big_data_ready_for_modeling.csv'\n",
    "df = spark.read.csv(local_file,\n",
    "                    header=True, inferSchema = True)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3140463-32f7-4ac0-b02b-59eb2eccef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['title','artist', 'album']\n",
    "# we are removing quotations from the titles\n",
    "df = df.select(\n",
    "    *[\n",
    "        F.regexp_replace(c, r\"[\\\"']\", \"\").alias(c)\n",
    "        if c in columns_to_clean else c\n",
    "        for c in df.columns\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1c798a-a4e3-4721-8329-78ff83b4792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| vectorized_features|\n",
      "+--------------------+\n",
      "|[0.666,0.83,-5.71...|\n",
      "|[0.818,0.803,-4.2...|\n",
      "|[0.849,0.759,-6.2...|\n",
      "|[0.731,0.863,-5.3...|\n",
      "|[0.745,0.875,-4.2...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "input_columns =  [\n",
    "    \"af_danceability\",\n",
    "    \"af_energy\",\n",
    "    \"af_loudness\",\n",
    "    \"af_speechiness\",\n",
    "    \"af_acousticness\",\n",
    "    \"af_instrumentalness\",\n",
    "    \"af_valence\",\n",
    "    \"af_tempo\",\n",
    "    \"language_id\"\n",
    "]\n",
    "vector_assembler = VectorAssembler(inputCols = input_columns, \n",
    "                            outputCol = 'vectorized_features')\n",
    "vector_dataframe = vector_assembler.transform(df) # df is our data without being vectorized\n",
    "transformed_vectors = vector_dataframe.select('vectorized_features') # selecting the vectorized features so we can use them in a model\n",
    "transformed_vectors.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b1da014-a639-496f-8346-2fb73ecf77bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|scaled_vectorized_features|\n",
      "+--------------------------+\n",
      "|      [7.30590220636553...|\n",
      "|      [8.97331532253304...|\n",
      "|      [9.31337983964615...|\n",
      "|      [8.01894070998979...|\n",
      "|      [8.17251823384733...|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "+--------------------+-----------------+--------------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+--------------------+--------------------------+\n",
      "|               title|           artist|               album|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|language_id| vectorized_features|scaled_vectorized_features|\n",
      "+--------------------+-----------------+--------------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+--------------------+--------------------------+\n",
      "|CANT STOP THE FEE...|Justin Timberlake|CANT STOP THE FEE...|          0.666|     0.83|     -5.715|        0.0751|         0.0123|                0.0|     0.702|  113.03|        0.0|[0.666,0.83,-5.71...|      [7.30590220636553...|\n",
      "+--------------------+-----------------+--------------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+--------------------+--------------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#scaling the data for k-means since it's a distance based algorithm \n",
    "scaler = StandardScaler(inputCol = 'vectorized_features',\n",
    "                       outputCol = 'scaled_vectorized_features',\n",
    "                       withStd = True,\n",
    "                       withMean = False)\n",
    "#check the summary statistics of our resutls by fitting the standard scaler\n",
    "scalerModel = scaler.fit(vector_dataframe)\n",
    "\n",
    "#normalizing each feature to have unit standard deviation\n",
    "vector_dataframe_scaled = scalerModel.transform(vector_dataframe)\n",
    "\n",
    "#showing the scaled features\n",
    "vector_dataframe_scaled.select('scaled_vectorized_features').show(5)\n",
    "vector_dataframe_scaled.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43be76bd-e08e-4d62-90db-15696ae56bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/02 13:11:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/12/02 13:11:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/12/02 13:11:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, -1234.2195756651586), (3, -971.6766521476645), (4, -712.8857740318316), (5, -737.6924212765223), (6, 1621.8753893362273), (7, 1878.1726614338481), (8, 2453.1373187199483), (9, 379.0702216518933), (10, 2520.2154976815673)]\n"
     ]
    }
   ],
   "source": [
    "results = [] \n",
    "for i in range(2,11): \n",
    "    gmm_model = GaussianMixture(k = i,\n",
    "                                featuresCol = 'scaled_vectorized_features', \n",
    "                                predictionCol = 'gmm_predictions') \n",
    "    model = gmm_model.fit(vector_dataframe_scaled) \n",
    "    lllh = model.summary.logLikelihood \n",
    "    results.append((i,lllh)) \n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95ab425-386b-4cb2-a118-cb3c1dbd321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K = 10\n"
     ]
    }
   ],
   "source": [
    "best_k = max(results, key=lambda x: x[1])[0]\n",
    "print(\"Best K =\", best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cc5d18-0778-4eed-af93-f99734427b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = best_k\n",
    "\n",
    "gmm = GaussianMixture(k = n_components, \n",
    "                      featuresCol = 'scaled_vectorized_features', \n",
    "                      predictionCol = 'gmm_predictions')\n",
    "model = gmm.fit(vector_dataframe_scaled)\n",
    "# predict where each song lies \n",
    "df_with_gmm_predictions = model.transform(vector_dataframe_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b869a9cd-c5f0-47e1-9452-7101d79529cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df  = df_with_gmm_predictions.filter(\"language_id = 0\")\n",
    "spanish_df  = df_with_gmm_predictions.filter(\"language_id = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63f3aefa-e951-4e94-b25d-1a36cb62b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "df_json_ready = df_with_gmm_predictions.withColumn(\"vectorized_features\", vector_to_array(\"vectorized_features\")).withColumn(\"scaled_vectorized_features\", vector_to_array(\"scaled_vectorized_features\")).withColumn(\"probability\", vector_to_array(\"probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4742ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"models/gmm_model\")\n",
    "scalerModel.write().overwrite().save(\"models/scaler_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a801cb3-234a-4391-86ac-54be4997c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for k in range(num_clusters):\n",
    "    \n",
    "#     # Filter cluster k\n",
    "#     cluster_df = df_json_ready.filter(f\"gmm_predictions = {k}\")\n",
    "    \n",
    "#     # English\n",
    "#     eng_rows = cluster_df.filter(\"language_id = 0\").toPandas().to_dict(orient=\"records\")\n",
    "    \n",
    "#     # Spanish\n",
    "#     spa_rows = cluster_df.filter(\"language_id = 1\").toPandas().to_dict(orient=\"records\")\n",
    "    \n",
    "#     # Build JSON object\n",
    "#     result = {\n",
    "#         f\"cluster_{k}\": {\n",
    "#             \"english\": eng_rows,\n",
    "#             \"spanish\": spa_rows\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     # Write JSON file\n",
    "#     with open(f\"{output_path}/cluster_{k}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     print(f\"Saved cluster_{k}.json\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e59d1f-fc75-4849-9edd-371a22879405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "num_clusters = best_k   # earlier we computed best_k\n",
    "\n",
    "#we have to create the folder before we run this\n",
    "\n",
    "output_path = \"gmm_cluster_outputs\"  # folder to store JSON files\n",
    "\n",
    "def df_to_dict_list(df):\n",
    "    return [row.asDict() for row in df.collect()]   # stays distributed\n",
    "\n",
    "for k in range(best_k):\n",
    "    cluster_df = df_json_ready.filter(f\"gmm_predictions = {k}\")\n",
    "    \n",
    "    eng = df_to_dict_list(cluster_df.filter(\"language_id = 0\"))\n",
    "    spa = df_to_dict_list(cluster_df.filter(\"language_id = 1\"))\n",
    "    \n",
    "    result = {\n",
    "        f\"cluster_{k}\": {\n",
    "            \"english\": eng,\n",
    "            \"spanish\": spa\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{output_path}/cluster_{k}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c37e62-354d-4592-bbb6-9fcb904b23fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- language_id: double (nullable = true)\n",
      " |-- vectorized_features: vector (nullable = true)\n",
      " |-- scaled_vectorized_features: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- gmm_predictions: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_gmm_predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b8d1b33-9830-45a1-b10c-ca4b75ed8805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|gmm_predictions|\n",
      "+---------------+\n",
      "|              2|\n",
      "|              7|\n",
      "|              4|\n",
      "|              0|\n",
      "|              0|\n",
      "|              0|\n",
      "|              4|\n",
      "|              0|\n",
      "|              0|\n",
      "|              0|\n",
      "+---------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_with_gmm_predictions.select('gmm_predictions').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81cde137-9c62-45f2-a5fe-838d37fbf8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|gmm_predictions|count|\n",
      "+---------------+-----+\n",
      "|              1|    4|\n",
      "|              6|    8|\n",
      "|              3|    5|\n",
      "|              5|    6|\n",
      "|              9|    4|\n",
      "|              4|    9|\n",
      "|              8|    5|\n",
      "|              7|    9|\n",
      "|              2|   22|\n",
      "|              0|  112|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of clusters\n",
    "df_with_gmm_predictions.groupBy('gmm_predictions').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ec0fbc9-2356-4586-a77c-cb729adf6a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+--------------------+--------------------------+--------------------+---------------+\n",
      "|               title|              artist|               album|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|language_id| vectorized_features|scaled_vectorized_features|         probability|gmm_predictions|\n",
      "+--------------------+--------------------+--------------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+--------------------+--------------------------+--------------------+---------------+\n",
      "|Bailando - Spanis...|Enrique Iglesias,...|SEX AND LOVE (Del...|          0.723|    0.777|     -3.503|         0.108|         0.0426|            3.68E-6|     0.961|  91.017|        0.0|[0.723,0.777,-3.5...|      [7.93118212492835...|[9.02296637618087...|              3|\n",
      "|Call On Me - Ryan...|             Starley|Call On Me (Remixes)|           0.67|    0.838|     -4.031|        0.0362|         0.0604|            6.11E-4|     0.717| 104.998|        0.0|[0.67,0.838,-4.03...|      [7.34978149889625...|[9.02296637615683...|              3|\n",
      "|Light It Up (feat...|         Major Lazer|Peace Is The Miss...|          0.746|    0.877|     -3.782|        0.0666|         0.0375|            8.33E-4|     0.751| 107.985|        0.0|[0.746,0.877,-3.7...|      [8.18348805698001...|[9.02296637617247...|              3|\n",
      "+--------------------+--------------------+--------------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+--------------------+--------------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show some language_id 0 songs in cluster 1\n",
    "df_english_cluster = df_with_gmm_predictions.filter((df_with_gmm_predictions.language_id == 0) & (df_with_gmm_predictions.gmm_predictions == 3)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26ddc2e5-2b2f-418a-8c95-7dab69ddaf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+-------------------+--------------------------+-----------+---------------+\n",
      "|title|artist|album|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|language_id|vectorized_features|scaled_vectorized_features|probability|gmm_predictions|\n",
      "+-----+------+-----+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+-------------------+--------------------------+-----------+---------------+\n",
      "+-----+------+-----+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+-----------+-------------------+--------------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cluster0_eng = df_with_gmm_predictions.filter((df_with_gmm_predictions.gmm_predictions == 0) & (df_with_gmm_predictions.language_id == 0))\n",
    "df_cluster0_eng.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc8c4c-af06-4112-b056-b3332b96adef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `vectorized_features` of the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_cluster0_eng\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/cluster0_eng\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `vectorized_features` of the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\"."
     ]
    }
   ],
   "source": [
    "# df_cluster0_eng.coalesce(1).write.csv(\n",
    "#     \"output/cluster0_eng\",\n",
    "#     header=True,\n",
    "#     mode=\"overwrite\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
