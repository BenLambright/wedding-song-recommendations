{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d5f8b4a6-8881-4fe2-a574-903633c5c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pyspark_hnsw as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pyspark_hnsw -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c881ac6-88dc-418a-9457-f0f5932a5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/Jelmerro/hnswlib-spark.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8931f75c-21f8-4941-bb0e-774a2a92c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark_hnsw.knn import HnswSimilarity, BruteForceSimilarity\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars.packages\", \"com.github.jelmerk:hnswlib-spark_3_5_2.12:2.0.0-beta.2\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef7513f6-8473-4adc-a9fb-523adedaf722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rank: double (nullable = true)\n",
      " |-- trend: double (nullable = true)\n",
      " |-- streams: double (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_key: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_mode: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_liveness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- af_time_signature: double (nullable = true)\n",
      " |-- language_id: double (nullable = true)\n",
      " |-- date: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('numpy_array_for_modeling.csv', header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a38a7f88-32d4-4a81-9432-f776edaf6b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c3744dc-1985-4ba6-9111-91c6d2012b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rank: double (nullable = true)\n",
      " |-- trend: double (nullable = true)\n",
      " |-- streams: double (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_key: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_mode: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_liveness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- af_time_signature: double (nullable = true)\n",
      " |-- language_id: double (nullable = true)\n",
      " |-- date: double (nullable = true)\n",
      "\n",
      "+----+-----+--------+----------+-----------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+-----------+------+\n",
      "|rank|trend| streams|popularity|duration_ms|af_danceability|af_energy|af_key|af_loudness|af_mode|af_speechiness|af_acousticness|af_instrumentalness|af_liveness|af_valence|af_tempo|af_time_signature|language_id|  date|\n",
      "+----+-----+--------+----------+-----------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+-----------+------+\n",
      "| 1.0|  1.0|253019.0|      78.0|   195840.0|          0.852|    0.773|   8.0|     -2.921|    0.0|        0.0776|          0.187|            3.05E-5|      0.159|     0.907| 102.034|              4.0|        1.0|2017.0|\n",
      "| 2.0|  2.0|223988.0|      72.0|   259195.0|          0.663|     0.92|  11.0|      -4.07|    0.0|         0.226|        0.00431|            1.69E-5|      0.101|     0.533|  99.935|              4.0|        1.0|2016.0|\n",
      "| 3.0|  0.0|210943.0|      73.0|   222560.0|          0.761|    0.838|   4.0|     -3.073|    0.0|        0.0502|            0.4|                0.0|      0.176|      0.71|  93.974|              4.0|        1.0|2016.0|\n",
      "| 4.0|  1.0|173865.0|       0.0|   205600.0|          0.508|    0.687|   0.0|     -4.361|    1.0|         0.326|          0.551|            3.41E-6|      0.126|     0.555| 180.044|              4.0|        1.0|2016.0|\n",
      "| 5.0|  2.0|153956.0|       0.0|   234320.0|          0.899|    0.626|   6.0|     -4.228|    0.0|         0.292|          0.076|                0.0|     0.0631|     0.873|  88.007|              4.0|        0.0|2016.0|\n",
      "+----+-----+--------+----------+-----------+---------------+---------+------+-----------+-------+--------------+---------------+-------------------+-----------+----------+--------+-----------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "16ba3441-d45d-4c27-8d10-0b2a551c90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an ID column because the algorithm needs it\n",
    "df = df.withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1bbcf332-ab04-46ca-9708-f6f0f6d179d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    \"rank\", \"trend\", \"streams\", \"popularity\", \"duration_ms\",\n",
    "    \"af_danceability\", \"af_energy\", \"af_key\", \"af_loudness\",\n",
    "    \"af_mode\", \"af_speechiness\", \"af_acousticness\",\n",
    "    \"af_instrumentalness\", \"af_liveness\", \"af_valence\",\n",
    "    \"af_tempo\", \"af_time_signature\", \"language_id\", \"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a47007a-917a-4ff6-b8bb-9738b75b6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols = feature_columns,\n",
    "    outputCol = 'featuresAsVectors'\n",
    ")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d525bb05-ee2c-48fc-9f9e-73db5821590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44d6fd3a-dddd-4852-90b6-0e48898ba099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('items')\n",
    "items = spark.table('items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f3a841b5-95fb-4a7b-a394-f37530bd657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol = 'features',\n",
    "                        outputCol = 'normalizedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6d5b8d4e-4728-43fa-a8be-ee4611f10b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "hnsw = HnswSimilarity(\n",
    "    identifierCol='id',\n",
    "    featuresCol='normalizedFeatures',\n",
    "    numPartitions=1,\n",
    "    numThreads=2,\n",
    "    k=4,\n",
    "    distanceFunction='cosine',\n",
    "    predictionCol='approximate' # the approximate predictions will be stored in this column\n",
    ")\n",
    "\n",
    "bruteForce = BruteForceSimilarity(\n",
    "    identifierCol= 'id',\n",
    "    featuresCol=\"normalizedFeatures\",\n",
    "    numPartitions=1,\n",
    "    numThreads=2,\n",
    "    k=4,\n",
    "    distanceFunction=\"inner-product\",\n",
    "    predictionCol='exact' # the exact predictions will be stored in this column\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5393b21-93f7-4971-adad-0d5e478f9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------\n",
    "# # 8. Build the complete pipeline\n",
    "# # ------------------------------\n",
    "# from pyspark.ml import Pipeline\n",
    "\n",
    "# pipeline = Pipeline(stages=[\n",
    "#     normalizer,\n",
    "#     hnsw,\n",
    "#     bruteForce\n",
    "# ])\n",
    "\n",
    "\n",
    "# # ------------------------------\n",
    "# # 9. Fit the HNSW + exact models\n",
    "# # ------------------------------\n",
    "# items = df\n",
    "\n",
    "# model = pipeline.fit(items)\n",
    "\n",
    "\n",
    "# # ------------------------------\n",
    "# # 10. Sample queries (exact kNN is expensive)\n",
    "# # ------------------------------\n",
    "# queries = items.sample(0.02)   # take 2% of rows\n",
    "\n",
    "\n",
    "# # ------------------------------\n",
    "# # 11. Run approximate + exact search\n",
    "# # ------------------------------\n",
    "# output = model.transform(queries)\n",
    "\n",
    "\n",
    "# # ------------------------------\n",
    "# # 12. Evaluate accuracy\n",
    "# # ------------------------------\n",
    "# from pyspark_hnsw.knn import KnnSimilarityEvaluator\n",
    "\n",
    "# evaluator = KnnSimilarityEvaluator(\n",
    "#     approximateNeighborsCol=\"approximate\",\n",
    "#     exactNeighborsCol=\"exact\"\n",
    "# )\n",
    "\n",
    "# accuracy = evaluator.evaluate(output)\n",
    "\n",
    "# print(\"Approximate kNN Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57f84f8b-106c-4450-9115-bcbb105d7f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rank: double (nullable = true)\n",
      " |-- trend: double (nullable = true)\n",
      " |-- streams: double (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- af_danceability: double (nullable = true)\n",
      " |-- af_energy: double (nullable = true)\n",
      " |-- af_key: double (nullable = true)\n",
      " |-- af_loudness: double (nullable = true)\n",
      " |-- af_mode: double (nullable = true)\n",
      " |-- af_speechiness: double (nullable = true)\n",
      " |-- af_acousticness: double (nullable = true)\n",
      " |-- af_instrumentalness: double (nullable = true)\n",
      " |-- af_liveness: double (nullable = true)\n",
      " |-- af_valence: double (nullable = true)\n",
      " |-- af_tempo: double (nullable = true)\n",
      " |-- af_time_signature: double (nullable = true)\n",
      " |-- language_id: double (nullable = true)\n",
      " |-- date: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Normalizer' from 'pyspark_hnsw.knn' (C:\\Users\\st114\\anaconda3\\Lib\\site-packages\\pyspark_hnsw\\knn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 59\u001b[0m\n\u001b[0;32m     49\u001b[0m converter \u001b[38;5;241m=\u001b[39m VectorConverter(\n\u001b[0;32m     50\u001b[0m     inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeaturesAsMlLibVector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     outputType\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray<float>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# 5. Normalize the feature vector\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark_hnsw\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Normalizer\n\u001b[0;32m     61\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer(\n\u001b[0;32m     62\u001b[0m     inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     63\u001b[0m     outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalizedFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     64\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# L2 normalization (default)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# 6. HNSW Approximate kNN\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Normalizer' from 'pyspark_hnsw.knn' (C:\\Users\\st114\\anaconda3\\Lib\\site-packages\\pyspark_hnsw\\knn.py)"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 1. Load CSV\n",
    "# ------------------------------\n",
    "df = spark.read.csv(\n",
    "    'numpy_array_for_modeling.csv',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df = df.na.drop()   # optional: drop rows with nulls\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Add a unique ID column\n",
    "# ------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Assemble all feature columns into ONE vector\n",
    "# ------------------------------\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\n",
    "    \"rank\", \"trend\", \"streams\", \"popularity\", \"duration_ms\",\n",
    "    \"af_danceability\", \"af_energy\", \"af_key\", \"af_loudness\",\n",
    "    \"af_mode\", \"af_speechiness\", \"af_acousticness\",\n",
    "    \"af_instrumentalness\", \"af_liveness\", \"af_valence\",\n",
    "    \"af_tempo\", \"af_time_signature\", \"language_id\", \"date\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"featuresAsMlLibVector\"\n",
    ")\n",
    "\n",
    "df = assembler.transform(df)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Convert MLlib vector â†’ array<float>\n",
    "# ------------------------------\n",
    "from pyspark_hnsw.conversion import VectorConverter\n",
    "\n",
    "converter = VectorConverter(\n",
    "    inputCol=\"featuresAsMlLibVector\",\n",
    "    outputCol=\"features\",\n",
    "    outputType=\"array<float>\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Normalize the feature vector\n",
    "# ------------------------------\n",
    "from pyspark_hnsw.knn import Normalizer\n",
    "\n",
    "normalizer = Normalizer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"normalizedFeatures\",\n",
    "    p=2  # L2 normalization (default)\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 6. HNSW Approximate kNN\n",
    "# ------------------------------\n",
    "from pyspark_hnsw.knn import HnswSimilarity\n",
    "\n",
    "hnsw = HnswSimilarity(\n",
    "    identifierCol=\"id\",\n",
    "    featuresCol=\"normalizedFeatures\",\n",
    "    numPartitions=1,\n",
    "    numThreads=4,\n",
    "    k=10,\n",
    "    distanceFunction=\"inner-product\",\n",
    "    predictionCol=\"approximate\",\n",
    "    m=48,\n",
    "    efConstruction=200\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Exact Brute-Force kNN\n",
    "# ------------------------------\n",
    "from pyspark_hnsw.knn import BruteForceSimilarity\n",
    "\n",
    "bruteForce = BruteForceSimilarity(\n",
    "    identifierCol=\"id\",\n",
    "    featuresCol=\"normalizedFeatures\",\n",
    "    numPartitions=1,\n",
    "    numThreads=4,\n",
    "    k=10,\n",
    "    distanceFunction=\"inner-product\",\n",
    "    predictionCol=\"exact\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Build the complete pipeline\n",
    "# ------------------------------\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    converter,\n",
    "    normalizer,\n",
    "    hnsw,\n",
    "    bruteForce\n",
    "])\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Fit the HNSW + exact models\n",
    "# ------------------------------\n",
    "items = df\n",
    "\n",
    "model = pipeline.fit(items)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Sample queries (exact kNN is expensive)\n",
    "# ------------------------------\n",
    "queries = items.sample(0.02)   # take 2% of rows\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 11. Run approximate + exact search\n",
    "# ------------------------------\n",
    "output = model.transform(queries)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 12. Evaluate accuracy\n",
    "# ------------------------------\n",
    "from pyspark_hnsw.knn import KnnSimilarityEvaluator\n",
    "\n",
    "evaluator = KnnSimilarityEvaluator(\n",
    "    approximateNeighborsCol=\"approximate\",\n",
    "    exactNeighborsCol=\"exact\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(output)\n",
    "\n",
    "print(\"Approximate kNN Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd15eb8-fe65-41a0-a322-b8d3a6615a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63984e85-7ada-4d1d-8582-1ef2c51831bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
