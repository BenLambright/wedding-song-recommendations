{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ddfeb504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "00cac0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language detection dependentcies\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6abfafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Preprocessing\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b3f2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = spark.read.csv(\"subset.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4fe10536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|               title|               album|language_id|\n",
      "+--------------------+--------------------+-----------+\n",
      "|Chantaje (feat. M...|           El Dorado|        1.0|\n",
      "|Vente Pa' Ca (fea...|Vente Pa' Ca (fea...|        1.0|\n",
      "|Reggaetón Lento (...|        Primera Cita|        1.0|\n",
      "|              Safari|             Energía|        1.0|\n",
      "|         Shaky Shaky|         Shaky Shaky|        0.0|\n",
      "+--------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# for now we'll do the language detection first, but this won't work at scale so we'll have to do somethign else later\n",
    "languages = [Language.ENGLISH, Language.SPANISH]\n",
    "pattern = r\"\\s*[\\(\\[].*?[\\)\\]]\"\n",
    "\n",
    "def detect_language_udf(title, album):\n",
    "    cleaned_title = re.sub(pattern, \"\", title, flags=re.IGNORECASE).strip()\n",
    "    cleaned_album = re.sub(pattern, \"\", album, flags=re.IGNORECASE).strip()\n",
    "    combined_text = f\"{cleaned_title} {cleaned_album}\"\n",
    "    if not combined_text.strip():\n",
    "        return 0.0  # return 0.0 for empty strings to just guess - we might want to make this more sophisticated later\n",
    "\n",
    "    detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "    language = detector.detect_language_of(combined_text)\n",
    "    return float(languages.index(language))\n",
    "\n",
    "detect_language = F.udf(detect_language_udf, FloatType())\n",
    "\n",
    "df_with_lang = df.withColumn(\"language_id\", detect_language(F.col(\"title\"), F.col(\"album\")))\n",
    "\n",
    "new_df = df_with_lang.select(\"title\", \"album\", \"language_id\")\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5524854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|Unnamed: 0|               title|              artist|explicit|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|\n",
      "+----------+--------------------+--------------------+--------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|         0|Chantaje (feat. M...|             Shakira|   false|          0.852|    0.773|     -2.921|        0.0776|          0.187|            3.05E-5|     0.907| 102.034|\n",
      "|         1|Vente Pa' Ca (fea...|        Ricky Martin|   false|          0.663|     0.92|      -4.07|         0.226|        0.00431|            1.69E-5|     0.533|  99.935|\n",
      "|         2|Reggaetón Lento (...|                CNCO|   false|          0.761|    0.838|     -3.073|        0.0502|            0.4|                0.0|      0.71|  93.974|\n",
      "|         3|              Safari|J Balvin, Pharrel...|   false|          0.508|    0.687|     -4.361|         0.326|          0.551|            3.41E-6|     0.555| 180.044|\n",
      "|         4|         Shaky Shaky|        Daddy Yankee|   false|          0.899|    0.626|     -4.228|         0.292|          0.076|                0.0|     0.873|  88.007|\n",
      "+----------+--------------------+--------------------+--------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# remove the following columns: urls, track_id, data, available markets, id, and date\n",
    "# also remove region, and name because they are strings not worth embedding for now\n",
    "# also remove chart because I don't think there are enough charts for this to be relevant\n",
    "# finally, remove index because dataframes already have an index\n",
    "columns_to_remove = [\n",
    "    \"urls\", \"track_id\", \"data\", \"available_markets\", \"id\", \"url\", \"date\",\n",
    "    \"region\", \"name\", \"chart\",\n",
    "    \"rank\", \"streams\", \"trend\", \"popularity\", \"duration_ms\", \"release_date\", \"af_time_signature\", \"af_key\", \"af_mode\", \"af_liveness\"\n",
    "]\n",
    "df = df.drop(*columns_to_remove)\n",
    "\n",
    "# df = df.drop(*columns_to_remove)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db4fde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates by checking to see if any titles match\n",
    "df = df.dropDuplicates([\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "22e7f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+--------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|Unnamed: 0|               title|           artist|explicit|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|\n",
      "+----------+--------------------+-----------------+--------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|        34|\"CAN'T STOP THE F...|Justin Timberlake|   false|          0.666|     0.83|     -5.715|        0.0751|         0.0123|                0.0|     0.702|  113.03|\n",
      "|        33|           24K Magic|       Bruno Mars|   false|          0.818|    0.803|     -4.282|        0.0797|          0.034|                0.0|     0.632|  106.97|\n",
      "|       114|                 743|         Miranda!|   false|          0.849|    0.759|     -6.232|        0.0304|         0.0951|            1.21E-4|     0.948| 110.983|\n",
      "|       163|           Acá Estoy|          El Reja|   false|          0.731|    0.863|     -5.331|        0.0377|          0.125|                0.0|     0.963| 107.992|\n",
      "|       141|Acércate (feat. N...|     De La Ghetto|   false|          0.745|    0.875|     -4.231|        0.0444|         0.0659|             1.5E-6|     0.728|  88.007|\n",
      "+----------+--------------------+-----------------+--------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# convert the date columns into a float representing the year\n",
    "def date_to_year(date):\n",
    "    try:\n",
    "        return float(date.year)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "date_to_year_udf = F.udf(date_to_year, FloatType())\n",
    "# df = df.withColumn(\"date\", date_to_year_udf(F.col(\"release_date\")))\n",
    "# df = df.drop(\"release_date\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dcfa58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the trend column into a scale from 0-2\n",
    "# def trend_to_scale(trend):\n",
    "#     if trend == \"MOVE_UP\":\n",
    "#         return 2.0\n",
    "#     elif trend == \"MOVE_DOWN\":\n",
    "#         return 0.0\n",
    "#     else:\n",
    "#         return 1.0\n",
    "\n",
    "# trend_to_scale_udf = F.udf(trend_to_scale, FloatType())\n",
    "# df = df.withColumn(\"trend\", trend_to_scale_udf(F.col(\"trend\")))\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3d997fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|Unnamed: 0|               title|           artist|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|\n",
      "+----------+--------------------+-----------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|        34|\"CAN'T STOP THE F...|Justin Timberlake|          0.666|     0.83|     -5.715|        0.0751|         0.0123|                0.0|     0.702|  113.03|\n",
      "|        33|           24K Magic|       Bruno Mars|          0.818|    0.803|     -4.282|        0.0797|          0.034|                0.0|     0.632|  106.97|\n",
      "|       114|                 743|         Miranda!|          0.849|    0.759|     -6.232|        0.0304|         0.0951|            1.21E-4|     0.948| 110.983|\n",
      "|       163|           Acá Estoy|          El Reja|          0.731|    0.863|     -5.331|        0.0377|          0.125|                0.0|     0.963| 107.992|\n",
      "|       141|Acércate (feat. N...|     De La Ghetto|          0.745|    0.875|     -4.231|        0.0444|         0.0659|             1.5E-6|     0.728|  88.007|\n",
      "+----------+--------------------+-----------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# remove explicit content, and then remove the explicit column\n",
    "df = df.filter(F.col(\"explicit\") == False)\n",
    "df = df.drop(\"explicit\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fcfdadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the rank column to a float\n",
    "# df = df.withColumn(\"rank\", F.col(\"rank\").cast(FloatType()))\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64c6b075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|               title|           artist|af_danceability|af_energy|af_loudness|af_speechiness|af_acousticness|af_instrumentalness|af_valence|af_tempo|\n",
      "+--------------------+-----------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "|\"CAN'T STOP THE F...|Justin Timberlake|          0.666|     0.83|     -5.715|        0.0751|         0.0123|                0.0|     0.702|  113.03|\n",
      "|           24K Magic|       Bruno Mars|          0.818|    0.803|     -4.282|        0.0797|          0.034|                0.0|     0.632|  106.97|\n",
      "|                 743|         Miranda!|          0.849|    0.759|     -6.232|        0.0304|         0.0951|            1.21E-4|     0.948| 110.983|\n",
      "|           Acá Estoy|          El Reja|          0.731|    0.863|     -5.331|        0.0377|          0.125|                0.0|     0.963| 107.992|\n",
      "|Acércate (feat. N...|     De La Ghetto|          0.745|    0.875|     -4.231|        0.0444|         0.0659|             1.5E-6|     0.728|  88.007|\n",
      "+--------------------+-----------------+---------------+---------+-----------+--------------+---------------+-------------------+----------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"Unnamed: 0\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd02224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-11-30 17:39:00.171\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `album` cannot be resolved. Did you mean one of the following? [`artist`, `title`, `af_tempo`, `af_energy`, `af_loudness`]. SQLSTATE: 42703\", \"context\": {\"file\": \"line 1 in cell [76]\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o746.withColumn.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `album` cannot be resolved. Did you mean one of the following? [`artist`, `title`, `af_tempo`, `af_energy`, `af_loudness`]. SQLSTATE: 42703;\\n'Project [title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947, detect_language_udf(title#2921, 'album)#3473 AS language_id#3474]\\n+- Project [title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\\n   +- Project [Unnamed: 0#2920, title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\\n      +- Filter (explicit#2934 = false)\\n         +- Deduplicate [title#2921]\\n            +- Project [Unnamed: 0#2920, title#2921, artist#2924, explicit#2934, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\\n               +- Relation [Unnamed: 0#2920,title#2921,rank#2922,date#2923,artist#2924,url#2925,region#2926,chart#2927,trend#2928,streams#2929,track_id#2930,album#2931,popularity#2932,duration_ms#2933,explicit#2934,release_date#2935,available_markets#2936,af_danceability#2937,af_energy#2938,af_key#2939,af_loudness#2940,af_mode#2941,af_speechiness#2942,af_acousticness#2943,af_instrumentalness#2944,... 4 more fields] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:1283)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumns(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2187)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:1819)\\n\\tat org.apache.spark.sql.classic.Dataset.withColumn(Dataset.scala:232)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor90.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:1583)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `album` cannot be resolved. Did you mean one of the following? [`artist`, `title`, `af_tempo`, `af_energy`, `af_loudness`]. SQLSTATE: 42703;\n'Project [title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947, detect_language_udf(title#2921, 'album)#3473 AS language_id#3474]\n+- Project [title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\n   +- Project [Unnamed: 0#2920, title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\n      +- Filter (explicit#2934 = false)\n         +- Deduplicate [title#2921]\n            +- Project [Unnamed: 0#2920, title#2921, artist#2924, explicit#2934, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\n               +- Relation [Unnamed: 0#2920,title#2921,rank#2922,date#2923,artist#2924,url#2925,region#2926,chart#2927,trend#2928,streams#2929,track_id#2930,album#2931,popularity#2932,duration_ms#2933,explicit#2934,release_date#2935,available_markets#2936,af_danceability#2937,af_energy#2938,af_key#2939,af_loudness#2940,af_mode#2941,af_speechiness#2942,af_acousticness#2943,af_instrumentalness#2944,... 4 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlanguage_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtitle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43malbum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df.show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/pyspark/sql/classic/dataframe.py:1623\u001b[39m, in \u001b[36mDataFrame.withColumn\u001b[39m\u001b[34m(self, colName, col)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1620\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_COLUMN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1621\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col).\u001b[34m__name__\u001b[39m},\n\u001b[32m   1622\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `album` cannot be resolved. Did you mean one of the following? [`artist`, `title`, `af_tempo`, `af_energy`, `af_loudness`]. SQLSTATE: 42703;\n'Project [title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947, detect_language_udf(title#2921, 'album)#3473 AS language_id#3474]\n+- Project [title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\n   +- Project [Unnamed: 0#2920, title#2921, artist#2924, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\n      +- Filter (explicit#2934 = false)\n         +- Deduplicate [title#2921]\n            +- Project [Unnamed: 0#2920, title#2921, artist#2924, explicit#2934, af_danceability#2937, af_energy#2938, af_loudness#2940, af_speechiness#2942, af_acousticness#2943, af_instrumentalness#2944, af_valence#2946, af_tempo#2947]\n               +- Relation [Unnamed: 0#2920,title#2921,rank#2922,date#2923,artist#2924,url#2925,region#2926,chart#2927,trend#2928,streams#2929,track_id#2930,album#2931,popularity#2932,duration_ms#2933,explicit#2934,release_date#2935,available_markets#2936,af_danceability#2937,af_energy#2938,af_key#2939,af_loudness#2940,af_mode#2941,af_speechiness#2942,af_acousticness#2943,af_instrumentalness#2944,... 4 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"language_id\", detect_language(F.col(\"title\"), F.col(\"album\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008323c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+\n",
      "|               title|           artist|            features|\n",
      "+--------------------+-----------------+--------------------+\n",
      "|\"CAN'T STOP THE F...|Justin Timberlake|[0.666, 0.83, -5....|\n",
      "|           24K Magic|       Bruno Mars|[0.818, 0.803, -4...|\n",
      "|                 743|         Miranda!|[0.849, 0.759, -6...|\n",
      "|           Acá Estoy|          El Reja|[0.731, 0.863, -5...|\n",
      "|Acércate (feat. N...|     De La Ghetto|[0.745, 0.875, -4...|\n",
      "+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# final step: convert it to be a column like this: [title, arist, features]\n",
    "# df = df.withColumn(\"features\", F.array([F.col(c).cast(FloatType()) for c in df.columns if c not in [\"title\", \"artist\"]]))\n",
    "# df = df.select(\"title\", \"artist\", \"features\")\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c17c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+\n",
      "|               title|           artist|            features|\n",
      "+--------------------+-----------------+--------------------+\n",
      "|\"CAN'T STOP THE F...|Justin Timberlake|[0.666, 0.83, -5....|\n",
      "|           24K Magic|       Bruno Mars|[0.818, 0.803, -4...|\n",
      "|                 743|         Miranda!|[0.849, 0.759, -6...|\n",
      "|           Acá Estoy|          El Reja|[0.731, 0.863, -5...|\n",
      "|Acércate (feat. N...|     De La Ghetto|[0.745, 0.875, -4...|\n",
      "+--------------------+-----------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# df = df.withColumn(\n",
    "#     \"features\",\n",
    "#     F.concat(\n",
    "#         \"features\",\n",
    "#         F.array(detect_language(F.col(\"title\"), F.col(\"artist\")))   # appending language id calculation\n",
    "#     )\n",
    "# )\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12dcac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `features` of the type \"ARRAY<FLOAT>\". SQLSTATE: 0A000",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy_prepped.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/pyspark/sql/readwriter.py:2146\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   2128\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   2129\u001b[39m     compression=compression,\n\u001b[32m   2130\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2144\u001b[39m     lineSep=lineSep,\n\u001b[32m   2145\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2146\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/big_data/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `features` of the type \"ARRAY<FLOAT>\". SQLSTATE: 0A000"
     ]
    }
   ],
   "source": [
    "df.write.csv(\"numpy_prepped.csv\", header=True, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
